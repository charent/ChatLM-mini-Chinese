from threading import Thread
import platform
from typing import Union
import torch

from transformers import TextIteratorStreamer,PreTrainedTokenizerFast
from safetensors.torch import load_model

from accelerate import init_empty_weights, dispatch_model,load_checkpoint_in_model, load_checkpoint_and_dispatch
from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model

# import è‡ªå®šä¹‰ç±»å’Œå‡½æ•°
from model.chat_model import TextToTextModel
from utils.functions import json_to_dataclass, fixed_space

from config import InferConfig

class ChatBot:
    def __init__(self, infer_config: InferConfig) -> None:
        '''
        '''
        
        self.infer_config = infer_config
        
        model_config_class = json_to_dataclass(infer_config.model_config_file, 'ModelConfig')
        self.model_config = model_config_class()

        # file_name=Noneä¼šè‡ªåŠ¨ç”Ÿæˆä»¥å½“å‰æ—¥æœŸå‘½åçš„logæ–‡ä»¶å
        # self.logger = Logger('chat_logs', std_out=True, save2file=True, file_name=None)

         # åˆå§‹åŒ–tokenizer
        tokenizer = PreTrainedTokenizerFast.from_pretrained(infer_config.tokenizer_dir)
        self.tokenizer = tokenizer
        self.encode = tokenizer.encode_plus
        self.batch_decode = tokenizer.batch_decode
        self.batch_encode_plus = tokenizer.batch_encode_plus
        
        empty_model = None
        with init_empty_weights():
            empty_model = TextToTextModel(config=self.model_config, decoder_start_token_id=tokenizer.pad_token_id)

        if torch.cuda.device_count() >= 2 and platform.system().lower() == 'linux':
            # é‡åŒ–é…ç½®
            bnb_quantization_config = BnbQuantizationConfig(
                load_in_4bit=True, 
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True, 
                bnb_4bit_quant_type="nf4"
                )
            
            # åŠ è½½æ¨¡å‹
            model = load_and_quantize_model(
                    model=empty_model, 
                    weights_location=infer_config.model_file, 
                    bnb_quantization_config=bnb_quantization_config, 
                )
            
            # å¤šGPUåˆ†å‘
            self.model = dispatch_model(
                model=model,
                device_map='auto'
            )   
                
        else:
            try:
                self.model = load_checkpoint_and_dispatch(
                    model=empty_model,
                    checkpoint=infer_config.model_file,
                    device_map='auto',
                    dtype=torch.float16,
                )
            except Exception as e:
                # print(str(e), '`accelerate` load fail, try another load function.')
                model = TextToTextModel(config=self.model_config, decoder_start_token_id=tokenizer.pad_token_id)

                if  infer_config.model_file.endswith('.safetensors'):
                    # load safetensors
                    load_model(model.model, infer_config.model_file) 
                else:
                    # load torch checkpoint
                    model.load_state_dict(torch.load(infer_config.model_file))  
                self.model = model

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

        self.streamer = TextIteratorStreamer(tokenizer=tokenizer, clean_up_tokenization_spaces=True, skip_special_tokens=True)

    def stream_chat(self, input_txt: str) -> TextIteratorStreamer:
        '''
        æµå¼å¯¹è¯ï¼Œçº¿ç¨‹å¯åŠ¨åå¯è¿”å›ï¼Œé€šè¿‡è¿­ä»£streamerè·å–ç”Ÿæˆçš„æ–‡å­—ï¼Œä»…æ”¯æŒgreedy search
        '''
        encoded = self.encode(input_txt + '[EOS]')
        
        input_ids = torch.LongTensor([encoded.input_ids]).to(self.device)
        attention_mask = torch.LongTensor([encoded.attention_mask]).to(self.device)

        generation_kwargs = {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'max_seq_len': self.infer_config.max_seq_len,
            'streamer': self.streamer,
            'search_type': 'greedy',
        }

        thread = Thread(target=self.model.my_generate, kwargs=generation_kwargs)
        thread.start()
        
        return self.streamer
    
    def chat(self, input_txt: Union[str, list[str]] ) -> Union[str, list[str]]:
        '''
        éæµå¼ç”Ÿæˆï¼Œå¯ä»¥ä½¿ç”¨beam searchã€beam sampleç­‰æ–¹æ³•ç”Ÿæˆæ–‡æœ¬ã€‚
        '''
        if isinstance(input_txt, str):
            input_txt = [input_txt]
        elif not isinstance(input_txt, list):
            raise Exception('input_txt mast be a str or list[str]')
        
        # add EOS token
        input_txts = [f"{txt}[EOS]" for txt in input_txt]
        encoded = self.batch_encode_plus(input_txts,  padding=True)
        input_ids = torch.LongTensor(encoded.input_ids).to(self.device)
        attention_mask = torch.LongTensor(encoded.attention_mask).to(self.device)

        outputs = self.model.my_generate(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            max_seq_len=self.infer_config.max_seq_len,
                            search_type='greedy',
                        )

        outputs = self.batch_decode(outputs.cpu().numpy(),  clean_up_tokenization_spaces=True, skip_special_tokens=True)

        note = "æˆ‘æ˜¯ä¸€ä¸ªå‚æ•°å¾ˆå°‘çš„AIæ¨¡å‹ğŸ¥ºï¼ŒçŸ¥è¯†åº“è¾ƒå°‘ï¼Œæ— æ³•ç›´æ¥å›ç­”æ‚¨çš„é—®é¢˜ï¼Œæ¢ä¸ªé—®é¢˜è¯•è¯•å§ğŸ‘‹"
        outputs = [item if len(item) != 0 else note for item in outputs]

        # åˆ é™¤decodeå‡ºæ¥å­—ç¬¦é—´çš„ç©ºæ ¼
        outputs = [fixed_space(item) for item in outputs]

        return outputs[0] if len(outputs) == 1 else outputs
